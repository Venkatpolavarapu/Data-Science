{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimise_mul_param_XGBoost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMexSyJGCmQwOtaYbyC7Nuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venkatpolavarapu/Data-Science/blob/master/Optimise_mul_param_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCMEHsbnCrjE",
        "outputId": "cede641a-8ab7-4cf2-95fd-a1e78767e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "def Snippet_194():\n",
        "    print()\n",
        "    print(format('How to optimise multiple parameters in XGBoost','*^82'))\n",
        "\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # load libraries\n",
        "    from sklearn import datasets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    from matplotlib import pyplot\n",
        "    pyplot.style.use('ggplot')\n",
        "    import numpy\n",
        "\n",
        "    # load the iris datasets\n",
        "    dataset = datasets.load_wine()\n",
        "    X = dataset.data; y = dataset.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "    # grid search\n",
        "    model = XGBClassifier()\n",
        "    n_estimators = [100, 200, 300, 400, 500]\n",
        "    learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
        "    param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
        "    grid_result = grid_search.fit(X, y)\n",
        "\n",
        "    # summarize results\n",
        "    print(); print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    means = grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "\n",
        "    for mean, stdev, param in zip(means, stds, params):\n",
        "\t     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "         # plot results\n",
        "    pyplot.subplots(figsize=(12,12))\n",
        "    scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))\n",
        "\n",
        "    for i, value in enumerate(learning_rate):\n",
        "        pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\n",
        "    pyplot.legend()\n",
        "    pyplot.xlabel('n_estimators')\n",
        "    pyplot.ylabel('Log Loss')\n",
        "    pyplot.show()\n",
        "    pyplot.savefig('n_estimators_vs_learning_rate.png')\n",
        "\n",
        "Snippet_194()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "******************How to optimise multiple parameters in XGBoost******************\n",
            "\n",
            "Best: -0.113679 using {'learning_rate': 0.1, 'n_estimators': 400}\n",
            "-1.086808 (0.000541) with: {'learning_rate': 0.0001, 'n_estimators': 100}\n",
            "-1.075230 (0.001099) with: {'learning_rate': 0.0001, 'n_estimators': 200}\n",
            "-1.063877 (0.001675) with: {'learning_rate': 0.0001, 'n_estimators': 300}\n",
            "-1.052715 (0.002243) with: {'learning_rate': 0.0001, 'n_estimators': 400}\n",
            "-1.041738 (0.002801) with: {'learning_rate': 0.0001, 'n_estimators': 500}\n",
            "-0.989548 (0.005409) with: {'learning_rate': 0.001, 'n_estimators': 100}\n",
            "-0.895755 (0.010918) with: {'learning_rate': 0.001, 'n_estimators': 200}\n",
            "-0.813327 (0.015103) with: {'learning_rate': 0.001, 'n_estimators': 300}\n",
            "-0.741425 (0.018362) with: {'learning_rate': 0.001, 'n_estimators': 400}\n",
            "-0.679226 (0.022005) with: {'learning_rate': 0.001, 'n_estimators': 500}\n",
            "-0.454886 (0.034015) with: {'learning_rate': 0.01, 'n_estimators': 100}\n",
            "-0.251858 (0.062648) with: {'learning_rate': 0.01, 'n_estimators': 200}\n",
            "-0.178734 (0.087154) with: {'learning_rate': 0.01, 'n_estimators': 300}\n",
            "-0.150544 (0.103889) with: {'learning_rate': 0.01, 'n_estimators': 400}\n",
            "-0.137119 (0.110578) with: {'learning_rate': 0.01, 'n_estimators': 500}\n",
            "-0.115199 (0.114838) with: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            "-0.113697 (0.117214) with: {'learning_rate': 0.1, 'n_estimators': 200}\n",
            "-0.113682 (0.117198) with: {'learning_rate': 0.1, 'n_estimators': 300}\n",
            "-0.113679 (0.117197) with: {'learning_rate': 0.1, 'n_estimators': 400}\n",
            "-0.113680 (0.117197) with: {'learning_rate': 0.1, 'n_estimators': 500}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}